{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f67cfaa0-6db6-44d4-804e-6b130fa74796",
   "metadata": {},
   "source": [
    "# Predictive Modeling on Customer Spending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34be88f-567d-4de3-a76e-b75e9e57ed85",
   "metadata": {},
   "source": [
    "This project applies numeric prediction techniques to build a predictive model for customer spending predictions. The dataset contains data about whether or not different consumers made a purchase in response to a test mailing of a certain catalog and, in case of a purchase, how much money each consumer spent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce350ed-4844-45cc-9cc2-f72e189f9143",
   "metadata": {},
   "source": [
    "## Part 1 \n",
    "This part aim to build numeric prediction models that predict Spending based on the other available customer information (obviously, not including the Purchase attribute among the inputs!). Several different models were applied, including linear regression, k-NN, regression tree, SVM regreesion, Neural Network and ensembling models. I will explore each techniques and present the best result (best predictive model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f008315-7f16-4280-9918-714f24b2d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import root_mean_squared_error, make_scorer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6e06f50-b193-4274-9741-6df6e24f2062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sequence_number  US  source_a  source_c  source_b  source_d  source_e  \\\n",
      "0                1   1         0         0         1         0         0   \n",
      "1                2   1         0         0         0         0         1   \n",
      "2                3   1         0         0         0         0         0   \n",
      "3                4   1         0         1         0         0         0   \n",
      "4                5   1         0         1         0         0         0   \n",
      "\n",
      "   source_m  source_o  source_h  ...  source_x  source_w  Freq  \\\n",
      "0         0         0         0  ...         0         0     2   \n",
      "1         0         0         0  ...         0         0     0   \n",
      "2         0         0         0  ...         0         0     2   \n",
      "3         0         0         0  ...         0         0     1   \n",
      "4         0         0         0  ...         0         0     1   \n",
      "\n",
      "   last_update_days_ago  1st_update_days_ago  Web order  Gender=male  \\\n",
      "0                  3662                 3662          1            0   \n",
      "1                  2900                 2900          1            1   \n",
      "2                  3883                 3914          0            0   \n",
      "3                   829                  829          0            1   \n",
      "4                   869                  869          0            0   \n",
      "\n",
      "   Address_is_res  Purchase  Spending  \n",
      "0               1         1    127.87  \n",
      "1               0         0      0.00  \n",
      "2               0         1    127.48  \n",
      "3               0         0      0.00  \n",
      "4               0         0      0.00  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "\n",
    "df = pd.read_excel(\"/Users/shawnwang/Desktop/Predictive Analytics/HW3/HW3.xlsx\", sheet_name=\"All Data\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "766ac5e0-2824-4e01-b7ab-ac9f20b84ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "\n",
    "df = df.drop(columns=[\"sequence_number\"])\n",
    "\n",
    "X = df.iloc[:, :-2]\n",
    "y = df[\"Spending\"]\n",
    "\n",
    "# 80/20 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "binary_cols = [col for col in numeric_cols if X[col].nunique() == 2]\n",
    "continuous_cols = [col for col in numeric_cols if col not in binary_cols]\n",
    "\n",
    "# Standardize continuous columns only\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "X_train_scaled[continuous_cols] = scaler.fit_transform(X_train[continuous_cols])\n",
    "X_test_scaled[continuous_cols] = scaler.transform(X_test[continuous_cols])\n",
    "\n",
    "# Log transformation since spending is skewed\n",
    "# y_train_log = np.log1p(y_train)\n",
    "# y_test_log = np.log1p(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b8ceb2-ad10-4c1a-a1ba-df1b14a42084",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "369f97b4-4c32-4905-9f0a-1e4051e29739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Nested CV RMSE: 126.1608 ± 15.9257\n",
      "Linear Regression Test RMSE: 129.3107\n"
     ]
    }
   ],
   "source": [
    "lr_model = LinearRegression()\n",
    "\n",
    "# Nested CV setup (Inner CV for GridSearch and Outer CV for Model Evaluation)\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define scoring metrics (RMSE scorer)\n",
    "rmse_scorer = make_scorer(root_mean_squared_error)\n",
    "\n",
    "# Performe Nested Cross-Validation (Outer CV)\n",
    "lr_scores = cross_val_score(lr_model, X_train_scaled, y_train, cv=outer_cv, scoring=rmse_scorer)\n",
    "print(f\"Linear Regression Nested CV RMSE: {lr_scores.mean():.4f} ± {lr_scores.std():.4f}\")\n",
    "\n",
    "# Train the best model on full training set\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "print(f\"Linear Regression Test RMSE: {root_mean_squared_error(y_test, y_pred_lr):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a99d18-c13b-4efa-b07b-b2941fc762b5",
   "metadata": {},
   "source": [
    "#### k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0c2a557f-71c0-4be1-b8be-da697ad3056a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "k-NN Nested CV RMSE: 144.8318 ± 22.1507\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "k-NN Test RMSE: 159.6923\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# Define Hyperparameter Grid for Tuning\n",
    "knn_params = {\n",
    "    \"n_neighbors\": [3, 5, 7, 9],\n",
    "    \"weights\": [\"uniform\", \"distance\"]\n",
    "}\n",
    "\n",
    "# Nested CV setup (Inner CV for GridSearch and Outer CV for Model Evaluation)\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define scoring metrics (RMSE scorer)\n",
    "rmse_scorer = make_scorer(root_mean_squared_error)\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning (Inner CV); Nested Cross-Validation (Outer CV)\n",
    "knn_grid = GridSearchCV(knn, knn_params, scoring=rmse_scorer, cv=inner_cv, n_jobs=-1, verbose=1)\n",
    "knn_scores = cross_val_score(knn_grid, X_train_scaled, y_train, cv=outer_cv, scoring=rmse_scorer)\n",
    "print(f\"k-NN Nested CV RMSE: {knn_scores.mean():.4f} ± {knn_scores.std():.4f}\")\n",
    "\n",
    "# Train the best model on full training set\n",
    "knn_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_knn = knn_grid.predict(X_test_scaled)\n",
    "print(f\"k-NN Test RMSE: {root_mean_squared_error(y_test, y_pred_knn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb97d1a-be0b-4e60-b6ce-9ec3664ce603",
   "metadata": {},
   "source": [
    "#### Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3ec9bd1e-a8ec-49fc-a536-78ce199e53b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Regression Tree Nested CV RMSE: 181.7520 ± 27.8873\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Regression Tree Test RMSE: 184.3340\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Define Hyperparameter Grid for Tuning\n",
    "tree_params = {\n",
    "    \"max_depth\": [3, 5, 10, None],\n",
    "    \"min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Nested CV setup (Inner CV for GridSearch and Outer CV for Model Evaluation)\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define scoring metrics (RMSE scorer)\n",
    "rmse_scorer = make_scorer(root_mean_squared_error)\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning (Inner CV); Nested Cross-Validation (Outer CV)\n",
    "tree_grid = GridSearchCV(tree, tree_params, scoring=rmse_scorer, cv=inner_cv, n_jobs=-1, verbose=1)\n",
    "tree_scores = cross_val_score(tree_grid, X_train, y_train, cv=outer_cv, scoring=rmse_scorer)\n",
    "print(f\"Regression Tree Nested CV RMSE: {tree_scores.mean():.4f} ± {tree_scores.std():.4f}\")\n",
    "\n",
    "# Train the best model on full training set\n",
    "tree_grid.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_tree = tree_grid.predict(X_test)\n",
    "print(f\"Regression Tree Test RMSE: {root_mean_squared_error(y_test, y_pred_tree):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbec12d8-e9a2-4431-965a-ce09caeea6a7",
   "metadata": {},
   "source": [
    "#### SVM Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7fcf78dc-8027-47cc-9b74-a19bfee464ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "SVM Regression Nested CV RMSE: 204.8357 ± 19.7009\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "SVM Regression Test RMSE: 215.2204\n"
     ]
    }
   ],
   "source": [
    "svr = SVR()\n",
    "\n",
    "# Define Hyperparameter Grid for Tuning\n",
    "svr_params = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"epsilon\": [0.1, 0.2, 0.5],\n",
    "    \"kernel\": [\"rbf\", \"linear\"]\n",
    "}\n",
    "\n",
    "# Nested CV setup (Inner CV for GridSearch and Outer CV for Model Evaluation)\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define scoring metrics (RMSE scorer)\n",
    "rmse_scorer = make_scorer(root_mean_squared_error)\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning (Inner CV); Nested Cross-Validation (Outer CV)\n",
    "svr_grid = GridSearchCV(svr, svr_params, scoring=rmse_scorer, cv=inner_cv, n_jobs=-1, verbose=1)\n",
    "svr_scores = cross_val_score(svr_grid, X_train_scaled, y_train, cv=outer_cv, scoring=rmse_scorer)\n",
    "print(f\"SVM Regression Nested CV RMSE: {svr_scores.mean():.4f} ± {svr_scores.std():.4f}\")\n",
    "\n",
    "# Train the best model on full training set\n",
    "svr_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_svr = svr_grid.predict(X_test_scaled)\n",
    "print(f\"SVM Regression Test RMSE: {root_mean_squared_error(y_test, y_pred_svr):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf65cf89-255a-4ade-a3de-b8add096ac07",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "645b42ad-bb8f-4b90-b6f5-7cd0363b6730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Neural Network Nested CV RMSE: 124.5299 ± 15.2990\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Neural Network Test RMSE: 132.8257\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor(\n",
    "    max_iter=2000,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define Hyperparameter Grid for Tuning\n",
    "mlp_params = {\n",
    "    \"hidden_layer_sizes\": [(50,), (100,), (50, 50)],\n",
    "    \"activation\": [\"relu\"],\n",
    "    \"alpha\": [0.0001, 0.001],\n",
    "    \"learning_rate\": [\"constant\", \"adaptive\"]\n",
    "}\n",
    "\n",
    "# Nested CV setup (Inner CV for GridSearch and Outer CV for Model Evaluation)\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define scoring metrics (RMSE scorer)\n",
    "rmse_scorer = make_scorer(root_mean_squared_error)\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning (Inner CV); Nested Cross-Validation (Outer CV)\n",
    "mlp_grid = GridSearchCV(mlp, mlp_params, scoring=rmse_scorer, cv=inner_cv, n_jobs=-1, verbose=1)\n",
    "mlp_scores = cross_val_score(mlp_grid, X_train_scaled, y_train, cv=outer_cv, scoring=rmse_scorer)\n",
    "print(f\"Neural Network Nested CV RMSE: {mlp_scores.mean():.4f} ± {mlp_scores.std():.4f}\")\n",
    "\n",
    "# Train the best model on full training set\n",
    "mlp_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_mlp = mlp_grid.predict(X_test_scaled)\n",
    "print(f\"Neural Network Test RMSE: {root_mean_squared_error(y_test, y_pred_mlp):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c26bf62-c53b-42b9-a017-c50e6592b8db",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "113ee968-0361-4118-93b2-e982124d9f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Random Forest Nested CV RMSE: 131.7516 ± 19.1827\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Random Forest Test RMSE: 138.3140\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define Hyperparameter Grid for Tuning\n",
    "rf_params = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"min_samples_split\": [2, 5]\n",
    "}\n",
    "\n",
    "# Nested CV setup (Inner CV for GridSearch and Outer CV for Model Evaluation)\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define scoring metrics (RMSE scorer)\n",
    "rmse_scorer = make_scorer(root_mean_squared_error)\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning (Inner CV); Nested Cross-Validation (Outer CV)\n",
    "rf_grid = GridSearchCV(rf, rf_params, scoring=rmse_scorer, cv=inner_cv, n_jobs=-1, verbose=1)\n",
    "rf_scores = cross_val_score(rf_grid, X_train, y_train, cv=outer_cv, scoring=rmse_scorer)\n",
    "print(f\"Random Forest Nested CV RMSE: {rf_scores.mean():.4f} ± {rf_scores.std():.4f}\")\n",
    "\n",
    "# Train the best model on full training set\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_rf = rf_grid.predict(X_test)\n",
    "print(f\"Random Forest Test RMSE: {root_mean_squared_error(y_test, y_pred_rf):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647fb608-0072-4757-87cf-1b4196a4ad94",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e863cb2c-c335-4ff1-810c-a7df0d309653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Gradient Boosting Nested CV RMSE: 136.1910 ± 21.8745\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Gradient Boosting Test RMSE: 139.7450\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Define Hyperparameter Grid for Tuning\n",
    "gb_params = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"max_depth\": [3, 5]\n",
    "}\n",
    "\n",
    "# Nested CV setup (Inner CV for GridSearch and Outer CV for Model Evaluation)\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define scoring metrics (RMSE scorer)\n",
    "rmse_scorer = make_scorer(root_mean_squared_error)\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning (Inner CV); Nested Cross-Validation (Outer CV)\n",
    "gb_grid = GridSearchCV(gb, gb_params, scoring=rmse_scorer, cv=inner_cv, n_jobs=-1, verbose=1)\n",
    "gb_scores = cross_val_score(gb_grid, X_train, y_train, cv=outer_cv, scoring=rmse_scorer)\n",
    "print(f\"Gradient Boosting Nested CV RMSE: {gb_scores.mean():.4f} ± {gb_scores.std():.4f}\")\n",
    "\n",
    "# Train the best model on full training set\n",
    "gb_grid.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_gb = gb_grid.predict(X_test)\n",
    "print(f\"Gradient Boosting Test RMSE: {root_mean_squared_error(y_test, y_pred_gb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79cd21-133c-4948-a435-9e198b9f49ee",
   "metadata": {},
   "source": [
    "### Summary\n",
    "By comparing the nested CV RMSE of each model, it seems that Neural Network has the lowest Nested CV RMSE (124.53) with a close RMSE on test data (132.83), hence should be the better predictive model here. The performance was followed closely by Linear Regression. I've also tried ensemble model of neural network and linear regression below, which seemed to perform only slightly better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f37190e6-8054-439b-a0f3-16ffdd5fc7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Test RMSE: 130.6990\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get predictions from both models\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "y_pred_nn = mlp_grid.predict(X_test_scaled)\n",
    "\n",
    "# Step 2: Average predictions\n",
    "y_pred_ensemble = (y_pred_lr + y_pred_nn)/2\n",
    "\n",
    "# Step 3: Evaluate ensemble\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "ensemble_rmse = root_mean_squared_error(y_test, y_pred_ensemble)\n",
    "print(f\"Ensemble Test RMSE: {ensemble_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa461f-7c46-4d8c-80e5-6936709bcdb1",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "As a variation, I will create a separate “restricted” dataset (i.e., a subset of the original dataset), which includes only purchase records (i.e., where Purchase = 1), and build numeric prediction models to predict Spending for this restricted dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "22f0ad14-8c60-499a-a363-56926dbe6a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = pd.read_excel(\"HW3.xlsx\", sheet_name=\"All Data\")\n",
    "\n",
    "# Filter to only include customers who made a purchase\n",
    "df_p = df_p[df_p[\"Purchase\"] == 1]\n",
    "\n",
    "# Drop the sequence_number column (as before)\n",
    "df_p = df_p.drop(columns=[\"sequence_number\"])\n",
    "\n",
    "X = df_p.iloc[:, :-2]\n",
    "y = df_p[\"Spending\"]\n",
    "\n",
    "# 80/20 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "binary_cols = [col for col in numeric_cols if X[col].nunique() == 2]\n",
    "continuous_cols = [col for col in numeric_cols if col not in binary_cols]\n",
    "\n",
    "# Standardize continuous columns only\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "X_train_scaled[continuous_cols] = scaler.fit_transform(X_train[continuous_cols])\n",
    "X_test_scaled[continuous_cols] = scaler.transform(X_test[continuous_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f0b97462-cfcb-44c5-9d52-7af059c7b640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested CV setup (Inner CV for GridSearch and Outer CV for Model Evaluation)\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define scoring metrics (RMSE scorer)\n",
    "rmse_scorer = make_scorer(root_mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c71cf7d-3a05-4182-87a0-eb22227c434f",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1afeb06a-11e0-4c60-92b5-dfca52b402a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Nested CV RMSE: 157.8116 ± 20.8627\n",
      "Linear Regression Test RMSE: 189.4194\n"
     ]
    }
   ],
   "source": [
    "lr_model = LinearRegression()\n",
    "\n",
    "# Performe Nested Cross-Validation (Outer CV)\n",
    "lr_scores = cross_val_score(lr_model, X_train_scaled, y_train, cv=outer_cv, scoring=rmse_scorer)\n",
    "print(f\"Linear Regression Nested CV RMSE: {lr_scores.mean():.4f} ± {lr_scores.std():.4f}\")\n",
    "\n",
    "# Train the best model on full training set\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "print(f\"Linear Regression Test RMSE: {root_mean_squared_error(y_test, y_pred_lr):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899917c5-2ecd-4b15-927e-fbbb986ee279",
   "metadata": {},
   "source": [
    "#### k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6ebe9e62-890e-4caf-b461-3440d080fba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "k-NN Nested CV RMSE: 171.0356 ± 21.6982\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "k-NN Test RMSE: 191.4186\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# Define Hyperparameter Grid for Tuning\n",
    "knn_params = {\n",
    "    \"n_neighbors\": [3, 5, 7, 9],\n",
    "    \"weights\": [\"uniform\", \"distance\"]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning (Inner CV); Nested Cross-Validation (Outer CV)\n",
    "knn_grid = GridSearchCV(knn, knn_params, scoring=rmse_scorer, cv=inner_cv, n_jobs=-1, verbose=1)\n",
    "knn_scores = cross_val_score(knn_grid, X_train_scaled, y_train, cv=outer_cv, scoring=rmse_scorer)\n",
    "print(f\"k-NN Nested CV RMSE: {knn_scores.mean():.4f} ± {knn_scores.std():.4f}\")\n",
    "\n",
    "# Train the best model on full training set\n",
    "knn_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_knn = knn_grid.predict(X_test_scaled)\n",
    "print(f\"k-NN Test RMSE: {root_mean_squared_error(y_test, y_pred_knn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d79fac-c7bd-49dd-ae76-dcf4f5f318ac",
   "metadata": {},
   "source": [
    "#### Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4816e297-bc8f-4eda-9dd4-7692afb35fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Regression Tree Nested CV RMSE: 222.3281 ± 11.3222\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Regression Tree Test RMSE: 208.1318\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Define Hyperparameter Grid for Tuning\n",
    "tree_params = {\n",
    "    \"max_depth\": [3, 5, 10, None],\n",
    "    \"min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning (Inner CV); Nested Cross-Validation (Outer CV)\n",
    "tree_grid = GridSearchCV(tree, tree_params, scoring=rmse_scorer, cv=inner_cv, n_jobs=-1, verbose=1)\n",
    "tree_scores = cross_val_score(tree_grid, X_train, y_train, cv=outer_cv, scoring=rmse_scorer)\n",
    "print(f\"Regression Tree Nested CV RMSE: {tree_scores.mean():.4f} ± {tree_scores.std():.4f}\")\n",
    "\n",
    "# Train the best model on full training set\n",
    "tree_grid.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_tree = tree_grid.predict(X_test)\n",
    "print(f\"Regression Tree Test RMSE: {root_mean_squared_error(y_test, y_pred_tree):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b8fbd-402b-4fb5-ab46-9d8d2a004f7d",
   "metadata": {},
   "source": [
    "#### SVM Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f2bbdf4c-a3d5-4c8b-880e-cfb02252db78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "SVM Regression Nested CV RMSE: 210.4521 ± 33.9121\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "SVM Regression Test RMSE: 269.1500\n"
     ]
    }
   ],
   "source": [
    "svr = SVR()\n",
    "\n",
    "# Define Hyperparameter Grid for Tuning\n",
    "svr_params = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"epsilon\": [0.1, 0.2, 0.5],\n",
    "    \"kernel\": [\"rbf\", \"linear\"]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning (Inner CV); Nested Cross-Validation (Outer CV)\n",
    "svr_grid = GridSearchCV(svr, svr_params, scoring=rmse_scorer, cv=inner_cv, n_jobs=-1, verbose=1)\n",
    "svr_scores = cross_val_score(svr_grid, X_train_scaled, y_train, cv=outer_cv, scoring=rmse_scorer)\n",
    "print(f\"SVM Regression Nested CV RMSE: {svr_scores.mean():.4f} ± {svr_scores.std():.4f}\")\n",
    "\n",
    "# Train the best model on full training set\n",
    "svr_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_svr = svr_grid.predict(X_test_scaled)\n",
    "print(f\"SVM Regression Test RMSE: {root_mean_squared_error(y_test, y_pred_svr):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d36a2c2-4a54-4a66-b1cf-e13e591e72e0",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7f9ee67c-d700-4ddd-a68d-5ee90e66cc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Neural Network Nested CV RMSE: 167.6999 ± 15.6801\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Neural Network Test RMSE: 191.1333\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor(\n",
    "    max_iter=2000,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define Hyperparameter Grid for Tuning\n",
    "mlp_params = {\n",
    "    \"hidden_layer_sizes\": [(50,), (100,), (50, 50)],\n",
    "    \"activation\": [\"relu\"],\n",
    "    \"alpha\": [0.0001, 0.001],\n",
    "    \"learning_rate\": [\"constant\", \"adaptive\"]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning (Inner CV); Nested Cross-Validation (Outer CV)\n",
    "mlp_grid = GridSearchCV(mlp, mlp_params, scoring=rmse_scorer, cv=inner_cv, n_jobs=-1, verbose=1)\n",
    "mlp_scores = cross_val_score(mlp_grid, X_train_scaled, y_train, cv=outer_cv, scoring=rmse_scorer)\n",
    "print(f\"Neural Network Nested CV RMSE: {mlp_scores.mean():.4f} ± {mlp_scores.std():.4f}\")\n",
    "\n",
    "# Train the best model on full training set\n",
    "mlp_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_mlp = mlp_grid.predict(X_test_scaled)\n",
    "print(f\"Neural Network Test RMSE: {root_mean_squared_error(y_test, y_pred_mlp):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c1d0d-395b-46ee-9a9b-f9669eee08f1",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "de622c2d-8fa7-458b-83dc-9d2005f3f214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Random Forest Nested CV RMSE: 160.5808 ± 19.4517\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Random Forest Test RMSE: 183.5751\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define Hyperparameter Grid for Tuning\n",
    "rf_params = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"min_samples_split\": [2, 5]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning (Inner CV); Nested Cross-Validation (Outer CV)\n",
    "rf_grid = GridSearchCV(rf, rf_params, scoring=rmse_scorer, cv=inner_cv, n_jobs=-1, verbose=1)\n",
    "rf_scores = cross_val_score(rf_grid, X_train, y_train, cv=outer_cv, scoring=rmse_scorer)\n",
    "print(f\"Random Forest Nested CV RMSE: {rf_scores.mean():.4f} ± {rf_scores.std():.4f}\")\n",
    "\n",
    "# Train the best model on full training set\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_rf = rf_grid.predict(X_test)\n",
    "print(f\"Random Forest Test RMSE: {root_mean_squared_error(y_test, y_pred_rf):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb6f5cc-7098-484b-87b1-1d0c1dd8709d",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "012875fc-bcc7-46f8-87d7-20875332d2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Gradient Boosting Nested CV RMSE: 171.3641 ± 17.7902\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Gradient Boosting Test RMSE: 188.3235\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Define Hyperparameter Grid for Tuning\n",
    "gb_params = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"max_depth\": [3, 5]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning (Inner CV); Nested Cross-Validation (Outer CV)\n",
    "gb_grid = GridSearchCV(gb, gb_params, scoring=rmse_scorer, cv=inner_cv, n_jobs=-1, verbose=1)\n",
    "gb_scores = cross_val_score(gb_grid, X_train, y_train, cv=outer_cv, scoring=rmse_scorer)\n",
    "print(f\"Gradient Boosting Nested CV RMSE: {gb_scores.mean():.4f} ± {gb_scores.std():.4f}\")\n",
    "\n",
    "# Train the best model on full training set\n",
    "gb_grid.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_gb = gb_grid.predict(X_test)\n",
    "print(f\"Gradient Boosting Test RMSE: {root_mean_squared_error(y_test, y_pred_gb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf5f952-bd1e-4dae-a92b-bd2f5b541fc3",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Judging by the nested CV RMSE, the better performing models should be Linear Regression and Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ce36f-6fc4-4f59-8f1b-626910fd7709",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "For each predictive modeling technique, the following discusses the predictive performance differences between the models built for Part1 and Part2: which models exhibit better predictive performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bdd69a-c9ee-40f4-bf5d-91fd9bbbd07c",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "The main difference between Part1 and Part2 is that Part1 contains many data with zero spending. The size of the data is larger but skewed. While Part2 contains buyers only, which the dataset is smaller but with more variance.\n",
    "In general, models had poorer performance when dealing with Part2. This could be due to the highly varied spending, leading to a higher error. As for each model's difference:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c28e66-b2e1-48aa-b2e4-c6d5a5687e2c",
   "metadata": {},
   "source": [
    "#### Linear Regression: Part2 has high variance that the LR model probably could not capture very well, thus cause higher error.\n",
    "Part1\n",
    "Linear Regression Nested CV RMSE: 126.1608 ± 15.9257\n",
    "Linear Regression Test RMSE: 129.3107\n",
    "\n",
    "Part2\n",
    "Linear Regression Nested CV RMSE: 157.8116 ± 20.8627\n",
    "Linear Regression Test RMSE: 189.4194\n",
    "\n",
    "#### k-NN: k-NN can struggle more on noisy or multi-dimensional targets. Part2 can make k-NN (which is distance-based) less stable.\n",
    "Part1\n",
    "k-NN Nested CV RMSE: 144.8318 ± 22.1507\n",
    "k-NN Test RMSE: 159.6923\n",
    "\n",
    "Part2\n",
    "k-NN Nested CV RMSE: 171.0356 ± 21.6982\n",
    "k-NN Test RMSE: 191.4186\n",
    "\n",
    "#### Regression Tree: Regression Tree overfit more on smaller and high-variance datasets, leading the the result of Part2.\n",
    "Part1\n",
    "Regression Tree Nested CV RMSE: 181.7520 ± 27.8873\n",
    "Regression Tree Test RMSE: 184.3340\n",
    "\n",
    "Part2\n",
    "Regression Tree Nested CV RMSE: 222.3281 ± 11.3222\n",
    "Regression Tree Test RMSE: 208.1318\n",
    "\n",
    "#### SVM Regression: Same as the other models, SVM performed worse on task(b)\n",
    "task(a)\n",
    "SVM Regression Nested CV RMSE: 204.8357 ± 19.7009\n",
    "SVM Regression Test RMSE: 215.2204\n",
    "\n",
    "task(b)\n",
    "SVM Regression Nested CV RMSE: 210.4521 ± 33.9121\n",
    "SVM Regression Test RMSE: 269.1500\n",
    "\n",
    "#### Neural Network: Neural Network performed relatively better on Part1, but may suffer more on Part2 as smaller dataset and less consistency may prevent deep patterns.\n",
    "Part1\n",
    "Neural Network Nested CV RMSE: 124.5299 ± 15.2990\n",
    "Neural Network Test RMSE: 132.8257\n",
    "\n",
    "Part2\n",
    "Neural Network Nested CV RMSE: 167.6999 ± 15.6801\n",
    "Neural Network Test RMSE: 191.1333\n",
    "\n",
    "#### Random Forest: More stable between Part1 and Part2. Random Forest may be able to reduce variance by ensembling different trees, thus produce smoother and more generalized predictions.\n",
    "Part1\n",
    "Random Forest Nested CV RMSE: 131.7516 ± 19.1827\n",
    "Random Forest Test RMSE: 138.3140\n",
    "\n",
    "Part2\n",
    "Random Forest Nested CV RMSE: 160.5808 ± 19.4517\n",
    "Random Forest Test RMSE: 183.5751\n",
    "\n",
    "#### Gradient Boosting: May struggle with high variance of Part2\n",
    "Part1\n",
    "Gradient Boosting Nested CV RMSE: 136.1910 ± 21.8745\n",
    "Gradient Boosting Test RMSE: 139.7450\n",
    "\n",
    "Part2\n",
    "Gradient Boosting Nested CV RMSE: 171.3641 ± 17.7902\n",
    "Gradient Boosting Test RMSE: 188.3235\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926333c6-f4af-41ae-8bab-563a531012d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3ae008b-e722-4a75-8d81-ffac8132d765",
   "metadata": {},
   "source": [
    "#### Acknowledgement\n",
    "This project is inspired by and recreated from assignments from the Predictive Analytics course by Professor Yicheng Song of the UMN MSBA program"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
